## 1. 泰勒展开

## 2. SGD
### 2.1 batch gd

### 2.2 stochastic gd

### 2.3 mini-batch gd 

# 3. 动量
> 动量法引入动量项$m$和折扣因子$\gamma$
> 梯度更新公式为:
> $$
>   d_i \leftarrow \beta d_{i-1} + g(\theta_{i-1}) \\
\theta_i \leftarrow \theta_{i-1} - \alpha d_i
> $$
> $\gamma$用来控制历史梯度的留存率，$\gamma$越大，历史梯度对当前的影响就越大。

# 4. 牛顿动量
> 牛顿动量公式:
> $$
>   d_i \leftarrow \beta d_{i-1} + g(\theta_{i-1}-\alpha\beta d_{i-1}) \\
\theta_i \leftarrow \theta_{i-1} - \alpha d_i
> $$
> 对原始公式进行变换,得到等效公式:
> $$
>   d_i \leftarrow \beta d_{i-1} + g(\theta_{i-1}) + \beta [g(\theta_{i-1})-g(\theta_{i-2})] \\
\theta_i \leftarrow \theta_{i-1} - \alpha d_i
> $$
> 相较于动量法的更新方式,增加了$\beta [g(\theta_{i-1})-g(\theta_{i-2})]$项。  
> 它的直观含义很明显：如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那我就把预计要增大的部分提前加进来；如果相比上次变小了，也是类似的情况。  
> 当这次梯度比上次梯度变大时,该项变大,当这次梯度比上次变小时,该项变小。可以发现该项可以理解为目标函数的二阶导近似,所以牛顿动量法可以加速收敛。

**推导**


# 5. Adagrad
> $$
>  r \leftarrow r+g(\theta_{i})*g(\theta_{i})  \\
> \Delta \theta \leftarrow \frac{\epsilon}{\delta+\sqrt{r}} \odot g(\theta_i) \\
> \theta \leftarrow \theta + \Delta\theta
> $$ 
对于历史梯度累积较大的参数,下次更新时梯度较小；
历史梯度累积较大时,下次更新梯度较大。

# 6. RMSProp
> $$
>  r \leftarrow \rho r+(1-\rho)g(\theta_{i})*g(\theta_{i})  \\
> \Delta \theta \leftarrow \frac{\epsilon}{\delta+\sqrt{r}} \odot g(\theta_i) \\
> \theta \leftarrow \theta + \Delta\theta
> $$ 
> 由于nn模型中大部分为非凸环境下,rmsprop通过系数衰减掉较远的历史梯度。


# 7. Adam
RMSProp+动量  
**Adam**
> $$
> g(\theta_i) <- \nabla f_t(x_{t-1})\\
> m_t \leftarrow \beta_1 m_{t-1}+(1-\beta_1)g(\theta_i)  \\
> v_t \leftarrow \beta_2 v_{t-1}+(1-\beta_2)g(\theta_i) \odot g(\theta_i) \\
> \hat{m_t} \leftarrow m_t / (1-\beta_1^t) \\
> \hat{v_t} \leftarrow v_t / (1-\beta_2^t) \\
> \theta_t \leftarrow \theta_{t-1} + \alpha * \hat{m_t} / (\sqrt{\hat{v_t}}+\epsilon)
> $$

**$m_t,v_t$的初始化问题**  
TODO

**学习率衰减**
> $$
> lr = \frac{lr}{1+decay*iterations}
> $$

**AdamWeightDecay**
> $$
> g(\theta_i) <- \nabla f_t(x_{t-1})\\
> m_t \leftarrow \beta_1 m_{t-1}+(1-\beta_1)g(\theta_i)  \\
> v_t \leftarrow \beta_2 v_{t-1}+(1-\beta_2)g(\theta_i) \odot g(\theta_i) \\
> \hat{m_t} \leftarrow m_t / (1-\beta_1^t) \\
> \hat{v_t} \leftarrow v_t / (1-\beta_2^t) \\
> \theta_t \leftarrow \theta_{t-1} + \alpha * ( \hat{m_t} / (\sqrt{\hat{v_t}} +wx_{t-1})+\epsilon)
> $$ 

在sgd中, 权重衰减和l2loss基本等价  
在adam中，如果使用l2loss, 则$g(\theta_i)$从   
  
$g(\theta_i) <- \nabla f_t(x_{t-1})\\$   
变为  
  
$g(\theta_i) <- \nabla f_t(x_{t-1}) +wx_{t-1}\\$  
对于权重比较大的参数，梯度也会比较大,由于adam会除以历史梯度的累积,使得减去项偏小，这l2正则项越大的权重惩罚越大的目的不一致。  
而使用权重衰减,在最后更新权重时加入权重衰减项，当权重越大时，衰减也越大，可以达到l2正则的目的

**NAdam**


# Adam和SGDM