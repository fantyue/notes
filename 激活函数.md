## 1.Sigmoid
**表达式**:
$$
\alpha(x)=\frac{1}{1+e^{-x}}
$$

> 假设一个全连接网络最后一层激活函数为sigmoid,对于最后一层的网络权重$w^L$
> loss函数对$w^L$的导数为:
> $$\frac{\alpha L}{\alpha w^L}=\frac{\alpha L}{\alpha^L}\frac{\alpha^L}{z^L}\frac{z^L}{w^L}$$
> $$\frac{\alpha^L}{z^L}=\sigma^\prime(w * a^{L-1}+b)$$
> $$\sigma^\prime(x)=\sigma(x)(1-\sigma(x))=\frac{e^{-x}}{(e^{-x}+1)^2}$$
> - 当输入的x绝对值很大时,$\sigma^\prime(x)$趋向于0;当输入的x绝对值很小时,$\sigma^\prime(x)$最大也只能达到0.25,在网络层数加深后,链式法则梯度反向传播，容易发生梯度消失,且层数越前越容易发生这种情况。

## 2.ReLU(线性整流单元)
**表达式**:
$$
ReLU(x) = max(0, x) \\
~\\
ReLU^\prime(x)=\left\{
    \begin{aligned}
    1 && \text{if }x>0\\
    0 && \text{if }x\le0
\end{aligned}
\right.
$$

>
> - ReLU反向传播时，梯度要么是0要么是1；
> 会导致很多权重和偏置不会更新。  
> - 使用ReLU作为激活函数时,会引入很大的稀疏性。  
> 
> **优点**:  
> 1. 避免sigmoid梯度消失的问题。
> 2. 由于稀疏的特性以及不涉及sigmoid的复杂计算,时间和空间复杂度降低。  
> 
> **缺点**
> 1. 由于导数为0，网络中很多参数不会更新。
> 2. 不能避免梯度爆炸的问题。


## 3.ELU(指数整流单元)
**表达式**:
$$
ELU(x)=\left\{
\begin{aligned}
    x && \text{if }x>0\\
    \alpha(e^x-1) && \text{if }x\le0
\end{aligned}
\right.
$$
> **优点**:
> 1. 能避免relu稀疏的问题。
> 2. 能避免relu导数为0的问题。
> 3. 能得到负值输出,能帮助网络往正确的方向推动权重和偏置变化。
> 
> **缺点**
> 1. 包含指数计算,运算时间较长。
> 2. 无法避免梯度爆炸问题。
> 3. $\alpha$值无法学习。

## 4.LeakyReLU
**表达式**:
$$
ELU(x)=\left\{
\begin{aligned}
    x && \text{if }x>0\\
    \alpha x && \text{if }x\le0
\end{aligned}
\right.
$$
> **优点**:
> 1. 能避免relu稀疏的问题。
> 2. 能避免relu导数为0的问题。
> 3. 能得到负值输出,能帮助网络往正确的方向推动权重和偏置变化。
> 
> **缺点**
> 1. 不包含指数计算,运算时间优于elu。
> 2. 无法避免梯度爆炸问题。
> 3. $\alpha$值无法学习。

## 5.seLU
**表达式**:
$$
ELU(x)=\lambda
\left\{
\begin{aligned}
    x && \text{if }x>0\\
    \alpha (e^x-1) && \text{if }x\le0
\end{aligned}
\right. \\
~\\
\alpha \approx 1.673 \\
\lambda \approx 1.050
$$
> - 当x>0时,输出值为x乘以$\lambda$。  
> - 需要使用lecun_normal进行初始化;如果使用dropout,应当使用AlphaDropout。
> 
> **优点**:
> 1. 能避免relu稀疏的问题。
> 2. 能避免relu导数为0的问题。
> 3. 能得到负值输出,能帮助网络往正确的方向推动权重和偏置变化。
> 
> **缺点**
> 1. 不包含指数计算,运算时间优于elu。
> 2. 无法避免梯度爆炸问题。
> 3. $\alpha$值无法学习。

## GELU
**表达式**:
$$
GELU(x)=0.5x(1-tanh(\sqrt{2/\pi}(x+0.044715x^3)))
$$
