## 经验总结

1. 使用BERT+CRF，CRF层学习率应该比BERT层学习率大(一般为5~10倍)，
2. NER任务中，蒸馏模型效果不佳
3. NER任务中，输入层embedding多样性比加深模型、加入attention效果更好。
4. NER任务的解码方式，crf/指针网络/biaffine等不同解码方式效果差别不大。
5. NER任务转化为阅读理解任务，效果可能有提升，但会提高计算复杂度。
6. NER任务中，尽量不要引入嵌套实体。
7. Transformer不太适合直接用于NER任务。


## NER任务的改进经验
1. 规则+领域词典可以快速提升NER效果
2. 在输入层引入更丰富的特征可以有效的提升性能。如引入char、bigram、词典特征(FLAT)、词性特征，或者基于多种特征构建embedding。或者也可以基于领域词典预训练字、词向量。
3. 词汇增强(FLAT)
4. 可以使用指针网络+CRF构建多任务学习解决span过长的问题。
5. BERT在NER中的作用。
    1. 可以作为baseline。
    2. 在垂直领域使用bert，一般需要根据领域语料预训练bert。
    3. 在样本较少时，用bert作为初版模型可能效果更好，可以在后期语料积累足够时，迁移到其他模型。
6. 当数据较少时，可以尝试:
    1. 结合bert,使用半监督学习+置信度选择
    2. 实体词典+bert，半监督自训练。
7. NER对底层输入敏感，引入太多弱监督、半监督等方法生成的语料可能会导致性能下降。
8. 对于NER的类目不均衡问题，模型层面的方法:重采样、loss惩罚、focal loss、dice loss、ghm等。一般这类实体本身就是长尾实体，可以采用规则模版、词表进行数据增广、后匹配。
9. 如何处理脏数据:切分训练集后，在一部分上训练模型，在另一部分预测，取出各个模型输出标签不一致的数据反标。或者可以使用置信学习。
10. 对于实际场景中的ner,可以构建多层级、多粒度、多策略的NER系统。
11. 如果追求速度，可以使用label attention代替crf(LAN)

## NER任务流程
1. 基于规则模版、词典的初版模型
2. 积累、清洗数据，基于预训练得到初版模型
3. 基于初版模型，利用置信度选择(高置信度直接作为语料、中等置信度辅助返标)、半监督学习、主动学习等方法迭代模型。

## 需要注意的问题
1. 数据清洗
2. 底层输入和embedding的设计
3. 词典的获取


## 为什么原生Transformer不适合使用在NER任务中
**(TENER)**
Transformer中使用sinusoidal position embedding，形如：
设position embedding维度为d,在第t个token的第2i位上:
```math
PE_{t,2i}=sin(t/10000^{2i/d})
```
在第t个token的第2i+1位上:
```math
PE_{t,2i+1}=cos(t/10000^{2i/d}) 
```

sinusoidal position embedding具备一个天然性质，那就是：
> 对于固定偏移量K，$PE_{t+k}$可以表达为$PE_{t}$的线性变换。

即$PE^T_tPE_{t+k}=PE^T_{t-k}PE_t$
故而Transformer只具有距离感知，缺乏对方向性的感知，而相对的bilstm具有对方向的感知。


## 词汇增强

https://zhuanlan.zhihu.com/p/142615620

1. 词汇增强的目的
    基于字符的NER系统一般效果优于基于词汇的方法，但基于字符的NER系统没有利用到词汇信息，而引入词汇信息对于确定实体边界有重要作用。
    
2. 主要方法
    1. Dynamic Architecture：设计一个动态框架，能够兼容词汇输入
    2. Adaptive Embedding：基于词汇信息，构建自适应Embedding




## 置信学习

https://arxiv.org/abs/1911.00068

主要步骤:
> 1. Count：估计噪声标签和真实标签的联合分布
> 2. Clean：找出过滤掉错误样本
> 3. Re-Training：过滤掉错误样本，重新调整样本类别权重，重新训练


**1.Count：估计噪声标签和真实标签的联合分布**
>噪声标签(人工标注的标签):$\tilde{y}$,真实标签:$y^*$,样本总数为n,类别总数为m。
> 1. 通过交叉验证得到第i个样本在第j个类别下的概率$P[i][j]$
> 2. 计算每个类目j下的平均概率$t[j]$作为置信度阈值
> 3. 对于样本i,将其真实标签设为满足$P[i][j]>t[j]$的类目中的概率最大的类目,即$argmax_iP[i][j]$
> 4. 计算计数矩阵，计数矩阵的第i行第j列的值为噪声标签为i,真实标签为j的样本的总数。
> 5. 计算标定计数矩阵$\tilde{C}_{\tilde{y}=i,y^*=j}$
> 6. 估计噪声标签和真实标签的联合分布$Q_{\tilde{y}=i,y^*=j}$ 


```math
    Q_{\tilde{y}=i,y^*=j}=\frac{C_{\tilde{y}=i,y^*=j}}{\sum_{j\in{1,2,3,...,m}}C_{\tilde{y}=i,y^*=j}}*|X_{\tilde{y}=i}|
```

```math
    \hat{Q}_{\tilde{y}=i,y^*=j}=\frac{Q_{\tilde{y}=i,y^*=j}}{\sum_{i\in{1,2,3,...,m},j\in{1,2,3,...,m}}Q_{\tilde{y}=i,y^*=j}}
```


**2.Clean：找出并过滤掉错误样本**
筛选错误样本策略:
> 1. 选取$P[i][j]$最大概率对应的下标j与i不一致的样本
> 2. 选取计数矩阵中的非对角线元素
> 3. Prune by Class:从噪声标签为i的样本中按概率从小到大排序，选取$n*\sum_{j\in1,...,m:j \neq i}(\hat{Q}_{\tilde{y}=i,y^*=j})$
> 4. Prune by Noise Rate:对于非对角单元格，选取$n*\hat{Q}_{\tilde{y}=i,y^*=j}$个元素过滤
> 5. C+NR:选择同时满足3，4的元素
![89191fa01ec053da6907181678142cc5.jpeg](en-resource://database/3259:1)



## NER模型框架
1. 序列标注
    -  给sentence中的每一个token一个标签，可以视为序列上的分类方法。
2. 指针标注
    - 层叠式指针网络: 对于每一个标签，构建两个与sentence长度一致的tensor,预测每个位置是否是tag的起点和终点。
    - QA+指针网络，构建两个与sentence长度一致的tensor，对不同的实体构建不同的query，并采取指针标注。
3. 多头标注
    - 构建一个 seq_len * seq_len 的span矩阵， span[i][j]的数值代表sentence从i到j位置的是否是一个实体，以及其实体类型。
4. 片段排序
    - 枚举句子中所有的片段，和width embed和cls拼接后，过全连接层+softmax分类。


**嵌套实体问题**

- 指针网络，多头标注，片段排序方法本身就可以解决嵌套实体问题
- 序列标注方法
    1. 多标签标注，将每个位置输出一个标签转化为按设定的阈值选取标签，缺点是学习难度大，容易导致标签之间的以来关系不易被模型学习。
    2. 多个序列标注层，将一个序列标注层转化为多个序列标注层，每一个序列标注层值包含某一种Tag的BIO。缺点是增加了参数量


**易混淆实体问题**


**非连续实体问题**


